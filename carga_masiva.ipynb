{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/y64xy8cd6cd355kr26pngsrh0000gn/T/ipykernel_65027/686394886.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
      "/Users/gabrielchavez/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucursales encontradas: [43, 18, 143, 68, 315, 138, 33, 93, 211, 154, 118, 153, 86, 186, 204, 221, 247, 187, 255, 36, 19, 261, 147, 276, 53, 241, 73, 83, 172, 254, 281, 113, 291, 100, 66, 274, 13, 38, 301, 72, 183, 133, 251, 200, 140, 65, 166, 106, 269, 81, 115, 132, 201, 96, 15, 32, 206, 79, 126, 253, 45, 146, 181, 240, 151, 213, 215, 304, 300, 287, 176, 47, 24, 44, 76, 119, 179, 145, 51, 279, 169, 144, 205, 101, 212, 111, 112, 273, 1, 11, 41, 26, 31, 266, 58, 130, 244, 209, 219, 192, 105, 249, 162, 313, 239, 3, 39, 4, 23, 30, 71, 184, 252, 191, 124, 159, 258, 259, 326, 327, 90, 139, 64, 309, 91, 98, 54, 158, 37, 5, 171, 262, 312, 245, 141, 94, 302, 109, 104, 173, 84, 307, 134, 198, 34, 9, 290, 170, 305, 238, 77, 69, 50, 102, 59, 127, 222, 137, 16, 197, 177, 202, 63, 207, 325, 150, 256, 117, 120, 224, 271, 152, 70, 103, 167, 48, 88, 29, 12, 217, 267, 97, 199, 320, 56, 190, 165, 22, 122, 216, 149, 42, 27, 2, 92, 82, 129, 317, 324, 250, 67, 220, 270, 99, 52, 163, 203, 265, 116, 49, 131, 210, 260, 62, 8, 17, 35, 25, 75, 263, 60, 180, 314, 178, 289, 248, 196, 128, 323, 85, 185, 195, 246, 28, 6, 21, 296, 278, 303, 328, 78, 108, 142, 214, 316, 46, 135, 189, 155, 182, 110, 125, 283, 160, 61, 14, 293, 311, 55, 157, 268, 174, 242, 243, 80, 148, 123, 89, 175, 107, 10, 40, 20, 7, 193, 275, 223, 257, 57, 322, 87, 74, 297, 218, 208, 295, 168, 188, 161, 121, 114]\n",
      "         Fecha_Id  Hora  Dia_Semana  Cantidad_Transacciones  \\\n",
      "59813  2025-02-14     6           5                       5   \n",
      "59815  2025-02-14     7           5                      13   \n",
      "59810  2025-02-14     8           5                      28   \n",
      "59820  2025-02-14     9           5                      33   \n",
      "59811  2025-02-14    10           5                      39   \n",
      "\n",
      "       Personal_Necesario  Suc_Id  \n",
      "59813                   1       1  \n",
      "59815                   3       1  \n",
      "59810                   5       1  \n",
      "59820                   5       1  \n",
      "59811                   6       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import joblib\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ===============================\n",
    "# 1. Definir la arquitectura del modelo\n",
    "# ===============================\n",
    "class DotacionPersonalNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DotacionPersonalNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(4, 64)\n",
    "        self.layer2 = torch.nn.Linear(64, 32)\n",
    "        self.layer3 = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# ===============================\n",
    "# 2. Cargar el modelo y el scaler\n",
    "# ===============================\n",
    "model_path = 'model.pth'\n",
    "scaler_path = 'scaler.pkl'\n",
    "\n",
    "model = DotacionPersonalNN()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# ===============================\n",
    "# 3. Función para generar las predicciones masivas\n",
    "# ===============================\n",
    "def generar_predicciones_masivas(suc_ids, fecha_inicio, fecha_fin, model, scaler, db_connection_string):\n",
    "    try:\n",
    "        # Crear conexión a la base de datos (para lectura)\n",
    "        engine = create_engine(db_connection_string)\n",
    "\n",
    "        # Generar rango de fechas en formato 'YYYY-MM-DD'\n",
    "        fechas_a_validar = pd.date_range(fecha_inicio, fecha_fin).strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "        # Consultar las transacciones por sucursal y fecha\n",
    "        transacciones_query = \"\"\"\n",
    "        SELECT suc_id, fecha_id, hora_id, MAX(conteo_transacciones) AS max_transacciones\n",
    "        FROM BI_Kielsa_Hecho_ProyeccionVenta_SucCanHora\n",
    "        WHERE suc_id IN ({}) AND fecha_id BETWEEN '{}' AND '{}'\n",
    "        GROUP BY suc_id, fecha_id, hora_id\n",
    "        \"\"\".format(\",\".join(map(str, suc_ids)), fecha_inicio, fecha_fin)\n",
    "        transacciones = pd.read_sql(transacciones_query, engine)\n",
    "        \n",
    "        # Convertir la columna 'fecha_id' a string con el formato 'YYYY-MM-DD'\n",
    "        transacciones['fecha_id'] = pd.to_datetime(transacciones['fecha_id']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Lista para almacenar los resultados\n",
    "        resultados = []\n",
    "\n",
    "        for suc_id in suc_ids:\n",
    "            for fecha_validada in fechas_a_validar:\n",
    "                # Calcular el día de la semana (1=Lunes, 7=Domingo)\n",
    "                dia_semana = datetime.strptime(fecha_validada, \"%Y-%m-%d\").weekday() + 1\n",
    "\n",
    "                # Filtrar las transacciones de la sucursal y fecha actual\n",
    "                transacciones_dia = transacciones[\n",
    "                    (transacciones['suc_id'] == suc_id) &\n",
    "                    (transacciones['fecha_id'] == fecha_validada)\n",
    "                ]\n",
    "                \n",
    "                # Para cada registro de transacción, se genera la predicción\n",
    "                for _, transaccion in transacciones_dia.iterrows():\n",
    "                    hora = transaccion['hora_id']\n",
    "                    transacciones_totales = math.ceil(transaccion['max_transacciones'])\n",
    "\n",
    "                    # Preparar los datos de entrada para el modelo\n",
    "                    input_data = pd.DataFrame({\n",
    "                        'Suc_Id': [suc_id],\n",
    "                        'Dia_Semana': [dia_semana],\n",
    "                        'Hora': [hora],\n",
    "                        'Transacciones_Totales': [transacciones_totales]\n",
    "                    })\n",
    "                    \n",
    "                    input_scaled = scaler.transform(input_data.values)\n",
    "                    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).view(1, -1)\n",
    "\n",
    "                    # Predecir el personal necesario\n",
    "                    with torch.no_grad():\n",
    "                        personal_necesario = model(input_tensor).item()\n",
    "                        # Se aplica un redondeo: si la predicción es mayor o igual a 1.2 se redondea hacia arriba,\n",
    "                        # de lo contrario hacia abajo.\n",
    "                        personal_necesario = math.ceil(personal_necesario) if personal_necesario >= 1.2 else math.floor(personal_necesario)\n",
    "\n",
    "                    # Almacenar el resultado (aseguramos que al menos sea 1)\n",
    "                    resultados.append({\n",
    "                        \"Fecha_Id\": fecha_validada,\n",
    "                        \"Hora\": hora,\n",
    "                        \"Dia_Semana\": dia_semana,\n",
    "                        \"Cantidad_Transacciones\": transacciones_totales,\n",
    "                        \"Personal_Necesario\": max(1, personal_necesario),\n",
    "                        \"Suc_Id\": suc_id\n",
    "                    })\n",
    "\n",
    "        # Convertir la lista de resultados a DataFrame\n",
    "        resultados_df = pd.DataFrame(resultados)\n",
    "        return resultados_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ===============================\n",
    "# 4. Extraer los IDs de todas las sucursales (dinámicamente)\n",
    "# ===============================\n",
    "# Connection string para lectura (BI_FARINTER)\n",
    "read_connection_string = (\n",
    "    'mssql+pyodbc://'\n",
    "    'Angel_chavez:{}@172.16.2.227\\\\DWHFARINTERDEV/BI_FARINTER?'\n",
    "    'driver=ODBC+Driver+17+for+SQL+Server'.format(quote_plus('@ng3l_ch@v3z'))\n",
    ")\n",
    "\n",
    "# Crear conexión para extraer los suc_ids\n",
    "engine_read = create_engine(read_connection_string)\n",
    "branch_ids_query = \"SELECT DISTINCT suc_id FROM BI_Kielsa_Hecho_ProyeccionVenta_SucCanHora\"\n",
    "branch_ids_df = pd.read_sql(branch_ids_query, engine_read)\n",
    "suc_ids = branch_ids_df[\"suc_id\"].tolist()\n",
    "\n",
    "print(\"Sucursales encontradas:\", suc_ids)\n",
    "\n",
    "# ===============================\n",
    "# 5. Definir el rango de fechas (febrero y marzo)\n",
    "# ===============================\n",
    "fecha_inicio = \"2025-02-14\"\n",
    "fecha_fin   = \"2025-03-31\"\n",
    "\n",
    "# ===============================\n",
    "# 6. Generar las predicciones masivas para todas las sucursales\n",
    "# ===============================\n",
    "resultados_df = generar_predicciones_masivas(suc_ids, fecha_inicio, fecha_fin, model, scaler, read_connection_string)\n",
    "\n",
    "if resultados_df is not None:\n",
    "    # ordenar por sucursal, dia, hora de menor a mayor\n",
    "    resultados_df = resultados_df.sort_values(by=['Suc_Id', 'Fecha_Id', 'Hora'])\n",
    "    print(resultados_df.head())\n",
    "    # save the results\n",
    "    resultados_df.to_csv('resultados.csv', index=False)\n",
    "else:\n",
    "    print(\"No se generaron resultados.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los datos se han insertado correctamente en IA_Farinter_PrediccionPersonal.\n"
     ]
    }
   ],
   "source": [
    "write_connection_string = (\n",
    "    'mssql+pyodbc://'\n",
    "    'Angel_chavez:{}@172.16.2.227\\\\DWHFARINTERDEV/IA_FARINTER?'\n",
    "    'driver=ODBC+Driver+17+for+SQL+Server'.format(quote_plus('@ng3l_ch@v3z'))\n",
    ")\n",
    "\n",
    "engine_write = create_engine(write_connection_string)\n",
    "\n",
    "# Insertar los resultados en la tabla. Si la tabla ya existe, se agrega (append).\n",
    "resultados_df.to_sql('IA_Farinter_PrediccionPersonal', engine_write, if_exists='append', index=False)\n",
    "\n",
    "print(\"Los datos se han insertado correctamente en IA_Farinter_PrediccionPersonal.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
